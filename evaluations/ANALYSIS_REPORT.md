# Evaluation Analysis Report

Generated: analyze_evaluations.py

## 1. Inter-Rater Reliability (Cohen's Kappa)

âš  Cannot compute yet - need common rated clauses from both evaluators.

## 2. WIT Model Quality

This measures how well the WIT clauses reflect the actual theses.

### Evaluator 1

| Rating | Count | Percentage |
|--------|-------|------------|
| Accept |  42 |  85.7% |
| Revise |   6 |  12.2% |
| Reject |   1 |   2.0% |
| **Total** | **49** | **100.0%** |

**Model Error Rate: 14.3%** (clauses needing revision/rejection)

## 3. Disagreement Analysis

No disagreements found.

## 5. Recommendations

- Continue with current evaluation process.
